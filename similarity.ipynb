{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing target domain rank...\n",
      "reading target domain and tokenization word2vecTrainCorpus.txt\n",
      "numeber of tokens: 532526\n",
      "computing source domain frequnecy...\n",
      "loading source model...\n",
      "wv2glove.6B.300d.bin model dimentional is 300\n",
      "number of dictionary: 400000\n",
      "_similarity_score.txt has been saved\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/Victor0118/cross_domain_embedding\n",
    "class zipf(object):\n",
    "    def __init__(self, ranks):\n",
    "        self.ranks= ranks\n",
    "        \n",
    "    def frequency(self):\n",
    "        #print (self.rank)        \n",
    "        f= {}\n",
    "        N= 0\n",
    "        \n",
    "        for word, rank in self.ranks:\n",
    "            freq= 1/rank\n",
    "            f[word]= freq\n",
    "            N += freq\n",
    "        \n",
    "        for word in f: f[word] /= N\n",
    "        #print(f)\n",
    "        return f\n",
    "\n",
    "#target\n",
    "import gensim\n",
    "import operator\n",
    "\n",
    "class CorpusRank(object): # normalized \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.MAX_SENTENSE_SIZE= 1000 # in general 15-20 words\n",
    "        \n",
    "    def iter(self):\n",
    "        print(\"reading target domain and tokenization\", self.filename)\n",
    "        for sentense in open(self.filename):\n",
    "            #print(sentense)\n",
    "            #if len(sentense) > self.MAX_SENTENSE_SIZE:    continue\n",
    "            for token in gensim.utils.tokenize(sentense, encoding='utf8',to_lower=True):    yield token \n",
    "            #yield line.strip().split(' ')\n",
    "                \n",
    "    def rank(self):\n",
    "        dic= {}\n",
    "        \n",
    "        print(\"computing target domain rank...\")\n",
    "        for word in self.iter():\n",
    "            #print(word)\n",
    "            if word not in dic:    dic[word]= 0\n",
    "            dic[word] += 1\n",
    "        \n",
    "        # sort by dictionary value \n",
    "        sorted_tuple = sorted(dic.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        \n",
    "        l_rank= []\n",
    "        rank= 1\n",
    "        for key, val in sorted_tuple:\n",
    "            l_rank.append( (key, rank) )\n",
    "            rank+= 1\n",
    "\n",
    "            \n",
    "        print(\"numeber of tokens:\",rank)\n",
    "        return l_rank\n",
    "\n",
    "#source\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "class WvRank(object): # normalized \n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        \n",
    "    def read_file(self):\n",
    "        print(\"loading source model...\")\n",
    "        model= KeyedVectors.load_word2vec_format(self.filename, binary=True)\n",
    "        print( self.filename + \" model dimentional is\", model.vector_size)\n",
    "        return model\n",
    "    \n",
    "    def rank(self):\n",
    "        dic= {}\n",
    "        rank= []       \n",
    "        print(\"computing source domain frequnecy...\")\n",
    "        for word, vocab_obj in self.read_file().vocab.items():   dic[word]= vocab_obj.count\n",
    "        \n",
    "        N= len(dic)\n",
    "        print(\"number of dictionary:\", N)\n",
    "        for word in dic: rank.append( (word, N+1-dic[word]) )\n",
    "        \n",
    "        return rank\n",
    "   \n",
    "\n",
    "class SimilarityScore(object): \n",
    "    def __init__(self, f1, f2, file_name):\n",
    "        self.f1 = f1\n",
    "        self.f2 = f2\n",
    "        self.file_name = file_name\n",
    "    \n",
    "    def save(self):\n",
    "        outF = open(self.file_name, \"w\")\n",
    "        score= {}\n",
    "        for w in f1:\n",
    "            if w in f2:\n",
    "                score[w]= 2*f1[w]*f2[w]/(f1[w] + f2[w])\n",
    "                outF.write('$'+ w +'$' + ' ' + '$'+ str(score[w])+'$')\n",
    "                outF.write(\"\\n\")\n",
    "        outF.close()\n",
    "        print(self.file_name, \"has been saved\")\n",
    "\n",
    "        \n",
    "target= 'word2vecTrainCorpus.txt'\n",
    "#target= 'corpus.txt'\n",
    "#target= '1000000_lines.txt'\n",
    "\n",
    "#source= 'GoogleNews-vectors-negative300.bin'        \n",
    "#source= '10000_lines.bin'\n",
    "source= 'wv2glove.6B.300d.bin' \n",
    "\n",
    "freq1= CorpusRank(target).rank()     \n",
    "freq2= WvRank(source).rank() \n",
    "\n",
    "f1, f2= zipf(freq1).frequency(), zipf(freq2).frequency()\n",
    "SimilarityScore(f1, f2, '_similarity_score.txt').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "./cre -train word2vecTrainCorpus.txt -model wv2glove.6B.300d.bin -similarity similarity_score.txt -output wv2glove.6B.300d_transfer_model.bin -size 300 -window 5 -binary 1 -lambda 10 -threads 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
